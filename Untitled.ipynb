{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/space/anaconda3/envs/cs6244-project/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/space/anaconda3/envs/cs6244-project/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/space/anaconda3/envs/cs6244-project/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/space/anaconda3/envs/cs6244-project/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/space/anaconda3/envs/cs6244-project/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/space/anaconda3/envs/cs6244-project/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "tf.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.13.1'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.random_normal(shape=[10, 32, 32, 3])\n",
    "\n",
    "conv1 = tf.layers.conv2d(x, 3, [2, 2], padding='SAME', reuse=None, name='conv')\n",
    "conv2 = tf.layers.conv2d(x, 3, [2, 2], padding='SAME', reuse=True, name='conv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reshape(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in tf.global_variables():\n",
    "    print(x.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_filters = 3\n",
    "num_convs = 3\n",
    "\n",
    "def conv_fn(conv_in):\n",
    "    \"\"\"\n",
    "    Convnet feature extractor from the state observations.\n",
    "\n",
    "    Args:\n",
    "        conv_in:  B x H x W x D\n",
    "\n",
    "    Returns:\n",
    "        B x Dout matrix\n",
    "    \"\"\"\n",
    "    assert len(conv_in.shape) == 4\n",
    "    conv_out = tf.layers.conv2d(\n",
    "        inputs=conv_in,\n",
    "        filters=num_filters,\n",
    "        kernel_size=[2, 2],\n",
    "        padding=\"same\",\n",
    "        activation=tf.nn.leaky_relu,\n",
    "        name=\"conv_initial\"\n",
    "    )\n",
    "\n",
    "    for i in range(0, num_convs - 1):\n",
    "        padding = \"same\" if i < num_convs - 2 else \"valid\"\n",
    "        conv_out = tf.layers.conv2d(\n",
    "            inputs=conv_out,\n",
    "            filters=num_filters,\n",
    "            kernel_size=[2, 2],\n",
    "            padding=padding,\n",
    "            activation=tf.nn.leaky_relu,\n",
    "            name=\"conv_{}\".format(i)\n",
    "        )\n",
    "    out = tf.layers.flatten(conv_out)\n",
    "    assert len(out.shape) == 2, '{}'.format(out.shape)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    T = 10\n",
    "    x = tf.random_normal(shape=[10, T, 32, 32, 3])\n",
    "    _input = x[:,0,:,:]\n",
    "\n",
    "    batchsize, h_length, height, width, depth = x.shape\n",
    "    x = tf.reshape(x, shape=[batchsize * h_length, height, width, depth])\n",
    "\n",
    "    out = conv_fn(x)\n",
    "    out = conv_fn(_input)\n",
    "\n",
    "    main()\n",
    "\n",
    "for x in tf.global_variables():\n",
    "    print(x.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 5) (10, 5)\n"
     ]
    }
   ],
   "source": [
    "with tf.variable_scope(\"pi\", reuse=tf.AUTO_REUSE):\n",
    "    x = tf.random_normal(shape=[2, 5, 10])\n",
    "    pos_added, (sine, cosine, even_mask, joint_pos, pos, i) = add_positional_embedding(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_positional_embedding(embedding, name=0):\n",
    "    \"\"\"\n",
    "    :param embedding: A tf.Tensor of shape [B, T, E]\n",
    "    :param lang: A language code (either \"de\" or \"en\")\n",
    "    \"\"\"\n",
    "    T = tf.shape(embedding)[1]\n",
    "    E= embedding.get_shape().as_list()[2]\n",
    "\n",
    "    # Create grid\n",
    "    pos = tf.cast(tf.tile(tf.expand_dims(tf.range(T), axis=0),\n",
    "                        multiples=[E, 1]), tf.float32)  # [E, T]\n",
    "    i = tf.cast(tf.tile(tf.expand_dims(tf.range(E), axis=1),\n",
    "                        multiples=[1, T]), tf.float32)    # [E, T]\n",
    "    print(pos.shape, i.shape)\n",
    "\n",
    "    # Sine waves\n",
    "    sine = tf.sin(tf.divide(pos, tf.pow(float(10**4), tf.divide(i, E))))     # [E, T]\n",
    "    cosine = tf.cos(tf.divide(pos, tf.pow(float(10**4), tf.divide(i, E))))   # [E, T]\n",
    "\n",
    "    # Shift cosine by one position\n",
    "    cosine = tf.manip.roll(cosine, shift=1, axis=0)\n",
    "\n",
    "    # Alternate between waves depending on parity\n",
    "    even_mask = tf.equal(tf.mod(tf.range(E), 2), 0)   # [E]\n",
    "    joint_pos = tf.where(condition=even_mask, x=sine, y=cosine)     # [E, T]\n",
    "    joint_pos = tf.transpose(joint_pos)     # [T, E]\n",
    "\n",
    "    # Magnitude of positional embedding\n",
    "    gamma = tf.get_variable(name=\"gamma_%d\" % name,\n",
    "                          shape=[],\n",
    "                          initializer=tf.initializers.ones,   # initially the same as specified by the paper\n",
    "                          trainable=True,\n",
    "                          dtype=tf.float32)\n",
    "\n",
    "    # Apply positional encoding\n",
    "    return tf.add(embedding, gamma * joint_pos, name=\"composed_embedding_%d\"%name),  # [B, T, E]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.random_normal(shape=[10, T, 32, 32, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2100., 5700.], dtype=float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = np.arange(1, 13, dtype=np.float32)*100\n",
    "a = tf.constant(b,shape=[2, 2, 3])\n",
    "c = b.reshape([2,2,3])\n",
    "np.sum(np.sum(c, axis=-1), axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(3)])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.shape[-1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=851, shape=(2, 2, 3), dtype=float32, numpy=\n",
       "array([[[-1.46385   , -0.8783101 , -0.29277003],\n",
       "        [ 0.2927699 ,  0.87830997,  1.46385   ]],\n",
       "\n",
       "       [[-1.46385   , -0.8783102 , -0.2927699 ],\n",
       "        [ 0.2927699 ,  0.8783097 ,  1.46385   ]]], dtype=float32)>"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b =  np.array([[1, 2, 3], \n",
    "              [4, 5, 6],\n",
    "              [7, 8, 9],\n",
    "              [10, 11, 12]], dtype=np.float32) * 100\n",
    "a = tf.constant(b,shape=[2, 2, 3])\n",
    "tf.contrib.layers.layer_norm(\n",
    "    a, center=False, scale=True, activation_fn=None, reuse=None,\n",
    "    variables_collections=None, outputs_collections=None, trainable=True,\n",
    "    begin_norm_axis=1, begin_params_axis=1, scope=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow._api.v1.keras.layers' has no attribute 'LayerNormalization'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-69-6e7af49b8d26>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m               [10, 11, 12]], dtype=np.float32) * 100\n\u001b[1;32m      5\u001b[0m \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLayerNormalization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: module 'tensorflow._api.v1.keras.layers' has no attribute 'LayerNormalization'"
     ]
    }
   ],
   "source": [
    "b =  np.array([[1, 2, 3], \n",
    "              [4, 5, 6],\n",
    "              [7, 8, 9],\n",
    "              [10, 11, 12]], dtype=np.float32) * 100\n",
    "a = tf.constant(b,shape=[2, 2, 3])\n",
    "tf.keras.layers.LayerNormalization(axis=1)(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.70116959, -0.60100251,  2.20367586],\n",
       "       [-0.40066834, -0.30050125, -0.20033417]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.array([1, 2, 30, 4, 5, 6]) * 100\n",
    "((a-np.mean(a)) /np.std(a)).reshape([2,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=717, shape=(2, 2, 3), dtype=float32, numpy=\n",
       "array([[[-0.70116955, -0.60100245,  2.2036757 ],\n",
       "        [-0.40066832, -0.30050123, -0.20033413]],\n",
       "\n",
       "       [[-1.46385   , -0.8783102 , -0.2927699 ],\n",
       "        [ 0.2927699 ,  0.8783097 ,  1.46385   ]]], dtype=float32)>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b =  np.array([[1, 2, 30], \n",
    "              [4, 5, 6],\n",
    "              [7, 8, 9],\n",
    "              [10, 11, 12]], dtype=np.float32) * 100\n",
    "a = tf.constant(b,shape=[2, 2, 3])\n",
    "tf.contrib.layers.layer_norm(\n",
    "    a, center=False, scale=False, activation_fn=None, reuse=None,\n",
    "    variables_collections=None, outputs_collections=None, trainable=True,\n",
    "    begin_norm_axis=1, begin_params_axis=-1, scope=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 100.,  200.,  300.],\n",
       "        [ 400.,  500.,  600.]],\n",
       "\n",
       "       [[ 700.,  800.,  900.],\n",
       "        [1000., 1100., 1200.]]], dtype=float32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(3)])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.shape[-1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=100, shape=(2, 2, 3), dtype=float32, numpy=\n",
       "array([[[-1.46385   , -0.8783101 , -0.29277003],\n",
       "        [ 0.2927699 ,  0.87830997,  1.46385   ]],\n",
       "\n",
       "       [[-1.46385   , -0.8783102 , -0.2927699 ],\n",
       "        [ 0.2927699 ,  0.8783097 ,  1.46385   ]]], dtype=float32)>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.contrib.layers.layer_norm(\n",
    "    a, center=True, scale=False, activation_fn=None, reuse=None,\n",
    "    variables_collections=None, outputs_collections=None, trainable=True,\n",
    "    begin_norm_axis=1, begin_params_axis=-1, scope=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention(query, key, value,\n",
    "              query_length, key_length,\n",
    "              num_heads, head_dims):\n",
    "    \"\"\"\n",
    "     Multi-head attention\n",
    "     :param query: A tf.Tensor of shape [B, TQ, E]\n",
    "     :param key: A tf.Tensor of shape [B, TK, E]\n",
    "     :param value: A tf.Tensor of shape [B, TK, E]\n",
    "     :param num_heads: How many sets of weights are desired\n",
    "     :param head_dims: Projection size of each head\n",
    "    \"\"\"\n",
    "    with tf.name_scope(\"attention\"):\n",
    "\n",
    "        # Make shapes compatible for tf.Dense\n",
    "        B = tf.shape(query)[0]\n",
    "        embedding_size = query.get_shape().as_list()[2]\n",
    "\n",
    "        # Number of times steps\n",
    "        TQ = tf.shape(query)[1]\n",
    "        TK = tf.shape(key)[1]\n",
    "\n",
    "        query = tf.reshape(query, shape=[B * TQ, embedding_size], name=\"query\")   # [B * TQ, E]\n",
    "        key = tf.reshape(key, shape=[B * TK, embedding_size], name=\"key\")   # [B * TK, E]\n",
    "        value = tf.reshape(value, shape=[B * TK, embedding_size], name=\"value\")   # [B * TK, E]\n",
    "\n",
    "        # Linear projections & single attention\n",
    "        head_summaries = []\n",
    "        for head_id in range(num_heads):\n",
    "\n",
    "            # Apply three different linear projections\n",
    "            head_query = tf.layers.Dense(units=head_dims,\n",
    "                                         activation=None,\n",
    "                                         use_bias=False)(query)  # [B * TQ, head_dims]\n",
    "            head_key = tf.layers.Dense(units=head_dims,\n",
    "                                       activation=None,\n",
    "                                       use_bias=False)(key)  # [B * TK, head_dims]\n",
    "            head_value = tf.layers.Dense(units=head_dims,\n",
    "                                         activation=None,\n",
    "                                         use_bias=False)(value)  # [B * TK, head_dims]\n",
    "            # Reshape\n",
    "            head_query = tf.reshape(head_query, shape=[B, TQ, head_dims])  # [B, TQ, head_dims]\n",
    "            head_key = tf.reshape(head_key, shape=[B, TK, head_dims])  # [B, TK, head_dims]\n",
    "            head_value = tf.reshape(head_value, shape=[B, TK, head_dims])  # [B, TK, head_dims]\n",
    "\n",
    "            # Derive attention logits\n",
    "            head_attention_weights = tf.matmul(head_query, tf.transpose(head_key, perm=[0, 2, 1]))  # [B, TQ, head_dims] @ [B, head_dims, TK]-> [B, TQ, TK]\n",
    "\n",
    "            # Rescale\n",
    "            head_attention_weights = tf.divide(head_attention_weights, np.sqrt(head_dims))  # [B, TQ, TK] Each query has a score for every key (TK)\n",
    "\n",
    "            # Normalize weights # not dynamic\n",
    "            head_attention_weights = tf.nn.softmax(head_attention_weights, axis=-1) # [B, TQ, TK]\n",
    "            \n",
    "            # Apply weights to values\n",
    "            head_summary = tf.matmul(head_attention_weights, head_value)  # [B, TQ, TK] @ [B, TK, head_dims] -> [B, TQ, head_dims]\n",
    "\n",
    "            # Add to collection\n",
    "            head_summaries.append(head_summary)\n",
    "\n",
    "        # Contract list of Tensors\n",
    "        head_summaries = tf.stack(head_summaries, axis=2, name=\"head_summaries\")   # [B, TQ, num_head, head_dims]\n",
    "\n",
    "        # Project back to input embedding size (E)\n",
    "        head_summaries = tf.reshape(head_summaries,\n",
    "                                    shape=[B * TQ, num_heads * head_dims],\n",
    "                                    name=\"head_summaries_reshaped\")  # [B * TQ, H * head_dims]\n",
    "        output = tf.layers.Dense(units=embedding_size,\n",
    "                                 activation=None,\n",
    "                                 use_bias=False)(head_summaries) # [B * TQ, H * head_dims] x [1, H * head_dims, E] -> [B * TQ, E]\n",
    "        output = tf.reshape(output, shape=[B, TQ, embedding_size], name=\"attention\")   # [B, TQ, E]\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoder_layer(x, x_length,\n",
    "                  num_heads, head_dims,\n",
    "                  dense_dim, dense_activation,\n",
    "                  dropout_rate=0, is_training=None):\n",
    "    \"\"\"\n",
    "    Encoder layer\n",
    "    :param x: A tf.Tensor of shape [B, T, E]\n",
    "    :param x_length:  A tf.Tensor of shape [B]\n",
    "    :param num_heads: Number of attention heads\n",
    "    :param head_dims: Dimension of linear projection of each head\n",
    "    :param dense_dims: Dimension of linear projection between dense layers\n",
    "    :param dense_activation: Activation function of the dense layers\n",
    "    :param dropout_rate: How many neurons should be deactivated (between 0 and 1)\n",
    "    :param is_training: Whether we are in training or prediction mode\n",
    "    \n",
    "    :return: A tf.Tensor of shape [B, T, E]\n",
    "    \"\"\"\n",
    "\n",
    "    B = tf.shape(x)[0]\n",
    "    T = tf.shape(x)[1]\n",
    "    E = x.get_shape().as_list()[2]\n",
    "\n",
    "    \"\"\" First sub-layer (self attention) \"\"\"\n",
    "    layer_norm_opts = {\n",
    "        \"begin_norm_axis\": 2,\n",
    "        \"begin_params_axis\": 2,\n",
    "        \"scale\": False,\n",
    "        \"center\": True\n",
    "    }\n",
    "\n",
    "    # Residual\n",
    "    residual = x    # [B, T, E]\n",
    "\n",
    "    # Layer normalization\n",
    "    x = tf.contrib.layers.layer_norm(inputs=x, **layer_norm_opts)      # [B, T, E]\n",
    "\n",
    "    # Self attention\n",
    "    x = attention(query=x,\n",
    "                  key=x,\n",
    "                  value=x,\n",
    "                  query_length=x_length,\n",
    "                  key_length=x_length,\n",
    "                  num_heads=num_heads,\n",
    "                  head_dims=head_dims)  # [B, T, E]\n",
    "\n",
    "    # Dropout\n",
    "    x = tf.layers.Dropout(rate=dropout_rate, noise_shape=[B, 1, E])(x, training=is_training)     # [B, T, E]\n",
    "\n",
    "    # Residual connection\n",
    "    x = x + residual    # [B, T, E]\n",
    "\n",
    "    # Layer normalization\n",
    "    x = tf.contrib.layers.layer_norm(inputs=x, **layer_norm_opts)      # [B, T, E]\n",
    "\n",
    "    \"\"\" Second sub-layer (dense) \"\"\"\n",
    "\n",
    "    # Residual\n",
    "    residual = x    # [B, T, E]\n",
    "\n",
    "    # Reshape to make output compatible with tf.layers.Dense\n",
    "    x = tf.reshape(x, shape=[B * T, E])    # [B * T, E]\n",
    "\n",
    "    # Dense\n",
    "    x = tf.layers.Dense(units=dense_dim,\n",
    "                        use_bias=True,\n",
    "                        activation=dense_activation)(x)    # [B * T, dense_dim]\n",
    "    # Dropout\n",
    "    x = tf.layers.Dropout(rate=dropout_rate, noise_shape=[1, dense_dim])(x, training=is_training)     # [B * T, dense_dim]\n",
    "\n",
    "    # Dense\n",
    "    x = tf.layers.Dense(units=E,\n",
    "                        use_bias=True,\n",
    "                        activation=dense_activation)(x)     # [B * T, E]\n",
    "\n",
    "    # Dropout\n",
    "    x = tf.layers.Dropout(rate=dropout_rate, noise_shape=[1, E])(x, training=is_training)     # [B * T, E]\n",
    "\n",
    "    # Reshape again\n",
    "    x = tf.reshape(x, shape=[B, T, E])     # [B, T, E]\n",
    "\n",
    "    # Residual connection\n",
    "    x = x + residual     # [B, T, E]\n",
    "\n",
    "    # Layer normalization\n",
    "    x = tf.contrib.layers.layer_norm(inputs=x, **layer_norm_opts)     # [B, T, E]\n",
    "\n",
    "    return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dimension(3)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.shape[-1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
